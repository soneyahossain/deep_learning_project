{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_learning_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soneyahossain/deep_learning_project/blob/master/deep_learning_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k1QPMHshMTId",
        "colab": {}
      },
      "source": [
        "!unzip DATA.zip\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4Nsa37xZKDyG",
        "colab": {}
      },
      "source": [
        "import torch.utils.data\n",
        "import json, os\n",
        "from PIL import Image, ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class UVaBuildinglabel(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_path, annotation_filepath, transform = None):\n",
        "        super(UVaBuildinglabel, self).__init__()\n",
        "        \n",
        "        print('Loading data...')\n",
        "        data = pd.read_csv(annotation_filepath)              #json.load(open(annotation_filepath, 'rb'))\n",
        "        \n",
        "        self.transform = transforms.Compose(\n",
        "        [transforms.Resize(320),  # 1. Resize smallest side to 256.\n",
        "        transforms.RandomCrop(320), # 2. Crop random square of 224x224 pixels.\n",
        "        #transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor()])\n",
        "        \n",
        "        self.image_path = image_path    # storing image path. \n",
        "        \n",
        "        \n",
        "        # get the image pixel values and labels\n",
        "        image_ids = data.iloc[:, 0 ]\n",
        "        label_ids = data.iloc[:, 1]\n",
        "        label_names = data.iloc[:, 2]\n",
        "\n",
        "        self.image_ids = data.iloc[:, 0 ] #[img_name for (img_id, img_name) in paired_id_names]\n",
        "        self.image_labels = data.iloc[:, 1]   #[img_id for (img_id, img_name) in paired_id_names]\n",
        "        self.image_classes= data.iloc[:, 2]\n",
        "\n",
        "\n",
        "        print(image_ids[0])\n",
        "        print(label_ids[0])\n",
        "        print(label_names)\n",
        "\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        image_name = self.image_ids[index]\n",
        "        img_ = Image.open(open(os.path.join(self.image_path, image_name), 'rb'))\n",
        "        img_ = img_.convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img_ = self.transform(img_)\n",
        "        label_ = self.image_labels[index]\n",
        "        class_name= self.image_classes[index]\n",
        "        return img_, label_, image_name,class_name \n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "       \n",
        "      \n",
        "# You can unnormalize the transformation of torch's transforms.normalize \n",
        "# with this custom transformation.\n",
        "class UnNormalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "    def __call__(self, tensor):\n",
        "        for t, m, s in zip(tensor, self.mean, self.std):\n",
        "            t.mul_(s).add_(m)\n",
        "        return tensor\n",
        "\n",
        "trainset = UVaBuildinglabel('DATA', 'trainSet.csv')\n",
        "valset = UVaBuildinglabel('DATA', 'valSet.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9Q3eZMKne2ME",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = range(0, 413)\n",
        "\n",
        "for i in x:\n",
        "     my_image, my_label,_,class_name  = trainset.__getitem__(i)\n",
        "\n",
        "print(my_image.shape)\n",
        "print(my_label)\n",
        "\n",
        "# Show the image and its label.\n",
        "plt.figure()\n",
        "label = class_name\n",
        "plt.title(\"This picture has: \"+label)\n",
        "# Undo normalization of the pixel values.\n",
        "\n",
        "# Re-arrange dimensions so it is height x width x channels.\n",
        "plt.imshow(my_image.transpose(0,2).transpose(0,1));\n",
        "#plt.grid(False); plt.axis('off');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnO2R1IKob0c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "57eb102c-4218-498d-ca1a-29ecf4549b3a"
      },
      "source": [
        "import torchvision.transforms.functional as F\n",
        "\n",
        "image_index = 126  # Feel free to change this.\n",
        "\n",
        "classes = ['airport_terminal', 'arch', 'bathroom', 'butte', 'castle', \n",
        "           'classroom', 'dentists_office', 'hot_spring', 'jacuzzi', \n",
        "           'laundromat', 'lecture_room', 'lighthouse', 'mountain', \n",
        "           'sauna', 'server_room', 'shower', 'skyscraper', 'tower',\n",
        "           'tree_house', 'volcano']\n",
        "\n",
        "\n",
        "# 1. Datasets need to implement the __len__ method for this to work.\n",
        "print('This dataset has {0} training images'.format(len(trainset)))\n",
        "\n",
        "# 2. Datasets need to implement the  __getitem__ method for this to work.\n",
        "img, label,_,class_name = trainset[image_index]  # Returns image and label.\n",
        "\n",
        "print(label)\n",
        "print(image_index)\n",
        "print('Image {0} is a {1}'.format(image_index, classes[label]))\n",
        "print('Image size is {0}x{1}x{2}'.format(img.shape[0], img.shape[1], img.shape[2]))\n",
        "# All images have 1 channel x 28 rows x 28 columns.\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(img.transpose(0,2).transpose(0,1));\n",
        "plt.grid(False); plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This dataset has 414 training images\n",
            "7\n",
            "126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-2c550fded210>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Image {0} is a {1}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Image size is {0}x{1}x{2}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# All images have 1 channel x 28 rows x 28 columns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'classes' is not defined"
          ]
        }
      ]
    }
  ]
}